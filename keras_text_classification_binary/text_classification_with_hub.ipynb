{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "Adapted from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKY4XMc9o8iB"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "\n",
    "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
    "\n",
    "The tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
    "\n",
    "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
    "\n",
    "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Installation\n",
    "Execute the following from the `\\Scripts` directory of the Python installation.\n",
    "\n",
    "```\n",
    "pip install numpy\n",
    "pip install tensorflow\n",
    "pip install \"C:\\path\\googleapis_common_protos-1.51.0-py3-none-any.whl\"\n",
    "pip install typing\n",
    "pip install tensorflow-datasets\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "```\n",
    "\n",
    "Restart jupyter if it is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "#!pip install tensorflow-hub\n",
    "#!pip install tfds-nightly\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is available on [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) or on [TensorFlow datasets](https://www.tensorflow.org/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):\n",
    "\n",
    "The dataset is downloaded to `%userprofile%\\tensorflow_datasets\\imdb_reviews`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in train_data 15000\n"
     ]
    }
   ],
   "source": [
    "train_data_list = [x for x in train_data]\n",
    "print(\"Number of reviews in train_data\", len(train_data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "\n",
    "Let's print first 10 examples.\n",
    "\n",
    "I replaced the original code withcode that prints a short version of the first three reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is a big step down after the surprisingly enjoyable original. This sequel isn't nearly as fun as part one, and it instead spends too much time on\"\n",
      "b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it si\"\n",
      "b'Hood of the Living Dead had a lot to live up to even before the opening credits began. First, any play on \"...of the living dead\" invokes His Holiness'\n"
     ]
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "train_examples_batch\n",
    "printable_examples = [x.numpy()[:150] for x in train_examples_batch[:3]]\n",
    "for x in printable_examples:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFtaCHTdc-GY"
   },
   "source": [
    "Let's also print the first 10 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvAjVXOWc6Mj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 1, 0, 1, 0, 1, 1, 1, 0], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
    "\n",
    "* How to represent the text?\n",
    "* How many layers to use in the model?\n",
    "* How many *hidden units* to use for each layer?\n",
    "\n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "*   we don't have to worry about text preprocessing,\n",
    "*   we can benefit from transfer learning,\n",
    "*   the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In2nDpTLkgKa"
   },
   "source": [
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NUbzVeYkgcO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 128), dtype=float32, numpy=\n",
       "array([[ 1.1329565 , -0.03431067,  0.12162483,  0.23909318, -0.00590096,\n",
       "         0.01708133,  0.08015746, -0.24174148,  0.02650847, -0.04987909,\n",
       "         0.11247347, -0.5601333 , -0.19981343, -0.33171475, -0.21482827,\n",
       "        -0.0408943 , -0.07008895,  0.04761497, -0.4271519 ,  0.5916435 ,\n",
       "         0.20662342, -0.01224876,  0.02093347,  0.08682311,  0.1036322 ,\n",
       "        -0.27562743,  0.08033531, -0.1508889 , -0.21847422,  0.06831451,\n",
       "        -0.14723632, -0.01117621,  0.25246456,  0.10763772,  0.16948967,\n",
       "        -0.0666896 , -0.2209269 , -0.3792632 , -0.15618715,  0.20775793,\n",
       "        -0.24780062,  0.05174338,  0.04574278, -0.03784208,  0.09963844,\n",
       "         0.2984763 ,  0.04106063, -0.1306813 , -0.23743384,  0.07203843,\n",
       "         0.09253472, -0.13241911, -0.29098544,  0.1412372 , -0.20057504,\n",
       "        -0.02892039, -0.14496696, -0.11467611,  0.35817885,  0.137189  ,\n",
       "        -0.0982758 ,  0.04868827,  0.01760143, -0.08307212, -0.0034984 ,\n",
       "        -0.01027516, -0.0081893 , -0.44247296,  0.2515449 ,  0.17821017,\n",
       "        -0.19196567, -0.04467885, -0.17082648, -0.28057122,  0.06178378,\n",
       "         0.10077247, -0.1975656 , -0.21596523, -0.08994614,  0.01176248,\n",
       "        -0.08658692,  0.08007401, -0.12543306,  0.0446598 , -0.07307897,\n",
       "         0.06574065, -0.2238672 ,  0.17806454,  0.9005481 ,  0.55621994,\n",
       "        -0.11694178,  0.09314022, -0.1796698 , -0.18921837,  0.0216456 ,\n",
       "         0.16930452, -0.02732026, -0.26192495,  0.16722474, -0.07650191,\n",
       "        -0.01805014,  0.00977933,  0.35422808,  0.02008115, -0.01643209,\n",
       "        -0.36127663, -0.01664132, -0.02406877, -0.02935027,  0.14058073,\n",
       "        -0.4286448 ,  0.0289977 , -0.16992389, -0.15780234,  0.10164463,\n",
       "        -0.29527357, -0.10797171,  0.04040849, -0.0489212 , -0.11398666,\n",
       "         0.1413943 , -0.16509335, -0.0897059 , -0.10753043, -0.11777861,\n",
       "        -0.1695381 ,  0.19131625,  0.25214472],\n",
       "       [ 1.3993114 ,  0.07091825,  0.17626837,  0.43239227,  0.2668123 ,\n",
       "        -0.13159902, -0.09596487, -0.4550524 , -0.20217389, -0.07174226,\n",
       "        -0.02219776, -0.25716725, -0.29525122, -0.30407235,  0.02016517,\n",
       "         0.09048087, -0.16293113,  0.11812694, -0.17674565,  0.8115082 ,\n",
       "         0.06450329,  0.20442279, -0.10483374,  0.09975157, -0.25057843,\n",
       "        -0.06292376,  0.17285128, -0.01758104, -0.30186537, -0.0841127 ,\n",
       "        -0.11114637,  0.05814372,  0.25236475, -0.19417268,  0.22315884,\n",
       "        -0.13729134, -0.06824694, -0.341236  ,  0.05999772,  0.41810328,\n",
       "        -0.20128277, -0.11010186, -0.16626425, -0.30353653,  0.11330597,\n",
       "         0.31198156,  0.11556248, -0.09846803, -0.30257723, -0.10206842,\n",
       "         0.03853445,  0.29365614, -0.0263411 ,  0.19250494, -0.18318594,\n",
       "        -0.387843  , -0.02986103, -0.07963722,  0.1678581 , -0.05191521,\n",
       "         0.00169835,  0.22374938,  0.2580453 , -0.16934054, -0.1468873 ,\n",
       "         0.03166825,  0.04612743, -0.26917952,  0.07807065, -0.12946828,\n",
       "        -0.40279043,  0.21611816,  0.04066769, -0.1153921 ,  0.13480614,\n",
       "         0.0533815 ,  0.01338466, -0.2828704 ,  0.02679291,  0.0553114 ,\n",
       "         0.2822382 ,  0.05319275, -0.03557036,  0.06398428,  0.01789244,\n",
       "        -0.12110488, -0.3023702 ,  0.15235233,  0.9217616 ,  0.28769338,\n",
       "        -0.37357402,  0.16022815,  0.04541649, -0.24403265,  0.05417794,\n",
       "         0.17239033,  0.12251239, -0.10201354, -0.07567043, -0.0966015 ,\n",
       "         0.05389016,  0.11833748,  0.44211835,  0.08047371,  0.05612666,\n",
       "        -0.17100687, -0.2711482 ,  0.11586259, -0.334083  ,  0.1370273 ,\n",
       "        -0.6890031 ,  0.03334224, -0.2867712 , -0.13632278, -0.04118263,\n",
       "        -0.3916107 , -0.03816559,  0.08995713, -0.2392849 ,  0.15313514,\n",
       "         0.23271668, -0.02023195,  0.19865698,  0.23517253, -0.0146532 ,\n",
       "        -0.14265741, -0.19406287,  0.13939749],\n",
       "       [ 1.2946371 , -0.11253475,  0.05029635,  0.2367229 , -0.13244802,\n",
       "         0.02443308, -0.14652136, -0.28027052, -0.18048939,  0.18805891,\n",
       "         0.13685365, -0.49752113, -0.09074111, -0.4175201 , -0.28877217,\n",
       "         0.05546521, -0.2556944 , -0.18724218, -0.31757542,  1.2265577 ,\n",
       "         0.05485158,  0.16542311,  0.07241534,  0.00525282, -0.06909394,\n",
       "        -0.30875286,  0.07455752, -0.02353438, -0.28600615,  0.07911482,\n",
       "         0.03179615,  0.06833802,  0.12678264,  0.03676327,  0.06749427,\n",
       "        -0.13177454, -0.20647688, -0.40936542,  0.04382695,  0.5071922 ,\n",
       "        -0.07591803, -0.01022388, -0.11367724, -0.03348291,  0.40537632,\n",
       "         0.01219334,  0.21148013, -0.15177499, -0.1675088 ,  0.25807852,\n",
       "        -0.06687336,  0.18238536, -0.07498309,  0.18307114, -0.02346368,\n",
       "        -0.1321392 ,  0.03790202,  0.05352983, -0.04517508, -0.09385905,\n",
       "        -0.10957684,  0.21261135,  0.01746937, -0.23257476,  0.09728883,\n",
       "        -0.06119744,  0.05879438, -0.5455181 ,  0.64510447,  0.20823778,\n",
       "        -0.26956758,  0.14502303,  0.26874134, -0.03323599,  0.23889118,\n",
       "         0.07796904, -0.03574307, -0.3639748 , -0.15900753,  0.04273356,\n",
       "         0.04090342,  0.03733777, -0.22868969, -0.05473918, -0.14085442,\n",
       "        -0.25791603, -0.17344423,  0.18025579,  0.5614433 ,  0.18826233,\n",
       "        -0.10364699,  0.17972913,  0.21095553, -0.1817403 ,  0.2866041 ,\n",
       "         0.18437123,  0.20135565, -0.23521572,  0.22913568,  0.18358126,\n",
       "        -0.09725957,  0.08446427,  0.34726897,  0.20610821, -0.05446661,\n",
       "        -0.32405758, -0.04476469,  0.09620295, -0.23533134,  0.06375794,\n",
       "        -0.32683748,  0.01521569, -0.45230308, -0.05667076, -0.21794087,\n",
       "        -0.13954318, -0.05025814, -0.07399976, -0.03221476, -0.13015044,\n",
       "         0.22862671, -0.3144746 ,  0.21375646, -0.1815645 , -0.01689448,\n",
       "        -0.11951823,  0.00593521,  0.12622672]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Embeddings\n",
    "\n",
    "The https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1 `SavedModel 2.0` has a vocabulary of 20,000 tokens and one out-of-vocabulary bucket for unknown tokens. The result is a tensor with 20,001 rows and 20 columns. The https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1 `SavedModel 2.0` has a vocabulary of 973,771 tokens and 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(973771, 128)\n"
     ]
    }
   ],
   "source": [
    "hub_layer_variables = hub_layer.variables\n",
    "print(hub_layer_variables[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfSbV6igl1EH"
   },
   "source": [
    "Let's now build the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 128)               124642688 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 124,644,769\n",
      "Trainable params: 124,644,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "The layers are stacked sequentially to build the classifier:\n",
    "\n",
    "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    "3. The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n",
    "\n",
    "Let's compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
    "\n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
    "\n",
    "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
    "\n",
    "Now, configure the model to use an optimizer and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "30/30 [==============================] - 45s 2s/step - loss: 0.6521 - accuracy: 0.6617 - val_loss: 0.5976 - val_accuracy: 0.7612\n",
      "Epoch 2/2\n",
      "30/30 [==============================] - 44s 1s/step - loss: 0.5069 - accuracy: 0.8381 - val_loss: 0.4434 - val_accuracy: 0.8341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=2,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, take a look [here](https://www.tensorflow.org/tutorials/keras/basic_text_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of text_classification_with_hub.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv363ml001",
   "language": "python",
   "name": "venv363ml001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
