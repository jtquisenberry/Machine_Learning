{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "Adapted from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKY4XMc9o8iB"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "\n",
    "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
    "\n",
    "The tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
    "\n",
    "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
    "\n",
    "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Installation\n",
    "Execute the following from the `\\Scripts` directory of the Python installation.\n",
    "\n",
    "```\n",
    "pip install numpy\n",
    "pip install tensorflow\n",
    "pip install \"C:\\path\\googleapis_common_protos-1.51.0-py3-none-any.whl\"\n",
    "pip install typing\n",
    "pip install tensorflow-datasets\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "```\n",
    "\n",
    "Restart jupyter if it is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "#!pip install tensorflow-hub\n",
    "#!pip install tfds-nightly\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is available on [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) or on [TensorFlow datasets](https://www.tensorflow.org/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):\n",
    "\n",
    "The dataset is downloaded to `%userprofile%\\tensorflow_datasets\\imdb_reviews`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in train_data 15000\n"
     ]
    }
   ],
   "source": [
    "train_data_list = [x for x in train_data]\n",
    "print(\"Number of reviews in train_data\", len(train_data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "\n",
    "Let's print first 10 examples.\n",
    "\n",
    "I replaced the original code withcode that prints a short version of the first three reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is a big step down after the surprisingly enjoyable original. This sequel isn't nearly as fun as part one, and it instead spends too much time on\"\n",
      "b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it si\"\n",
      "b'Hood of the Living Dead had a lot to live up to even before the opening credits began. First, any play on \"...of the living dead\" invokes His Holiness'\n"
     ]
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "train_examples_batch\n",
    "printable_examples = [x.numpy()[:150] for x in train_examples_batch[:3]]\n",
    "for x in printable_examples:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFtaCHTdc-GY"
   },
   "source": [
    "Let's also print the first 10 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvAjVXOWc6Mj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 1, 0, 1, 0, 1, 1, 1, 0], dtype=int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
    "\n",
    "* How to represent the text?\n",
    "* How many layers to use in the model?\n",
    "* How many *hidden units* to use for each layer?\n",
    "\n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "*   we don't have to worry about text preprocessing,\n",
    "*   we can benefit from transfer learning,\n",
    "*   the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In2nDpTLkgKa"
   },
   "source": [
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NUbzVeYkgcO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 128), dtype=float32, numpy=\n",
       "array([[ 1.1329565 , -0.03431067,  0.12162483,  0.23909318, -0.00590096,\n",
       "         0.01708133,  0.08015746, -0.24174148,  0.02650847, -0.04987909,\n",
       "         0.11247347, -0.5601333 , -0.19981343, -0.33171475, -0.21482827,\n",
       "        -0.0408943 , -0.07008895,  0.04761497, -0.4271519 ,  0.5916435 ,\n",
       "         0.20662342, -0.01224876,  0.02093347,  0.08682311,  0.1036322 ,\n",
       "        -0.27562743,  0.08033531, -0.1508889 , -0.21847422,  0.06831451,\n",
       "        -0.14723632, -0.01117621,  0.25246456,  0.10763772,  0.16948967,\n",
       "        -0.0666896 , -0.2209269 , -0.3792632 , -0.15618715,  0.20775793,\n",
       "        -0.24780062,  0.05174338,  0.04574278, -0.03784208,  0.09963844,\n",
       "         0.2984763 ,  0.04106063, -0.1306813 , -0.23743384,  0.07203843,\n",
       "         0.09253472, -0.13241911, -0.29098544,  0.1412372 , -0.20057504,\n",
       "        -0.02892039, -0.14496696, -0.11467611,  0.35817885,  0.137189  ,\n",
       "        -0.0982758 ,  0.04868827,  0.01760143, -0.08307212, -0.0034984 ,\n",
       "        -0.01027516, -0.0081893 , -0.44247296,  0.2515449 ,  0.17821017,\n",
       "        -0.19196567, -0.04467885, -0.17082648, -0.28057122,  0.06178378,\n",
       "         0.10077247, -0.1975656 , -0.21596523, -0.08994614,  0.01176248,\n",
       "        -0.08658692,  0.08007401, -0.12543306,  0.0446598 , -0.07307897,\n",
       "         0.06574065, -0.2238672 ,  0.17806454,  0.9005481 ,  0.55621994,\n",
       "        -0.11694178,  0.09314022, -0.1796698 , -0.18921837,  0.0216456 ,\n",
       "         0.16930452, -0.02732026, -0.26192495,  0.16722474, -0.07650191,\n",
       "        -0.01805014,  0.00977933,  0.35422808,  0.02008115, -0.01643209,\n",
       "        -0.36127663, -0.01664132, -0.02406877, -0.02935027,  0.14058073,\n",
       "        -0.4286448 ,  0.0289977 , -0.16992389, -0.15780234,  0.10164463,\n",
       "        -0.29527357, -0.10797171,  0.04040849, -0.0489212 , -0.11398666,\n",
       "         0.1413943 , -0.16509335, -0.0897059 , -0.10753043, -0.11777861,\n",
       "        -0.1695381 ,  0.19131625,  0.25214472],\n",
       "       [ 1.3993114 ,  0.07091825,  0.17626837,  0.43239227,  0.2668123 ,\n",
       "        -0.13159902, -0.09596487, -0.4550524 , -0.20217389, -0.07174226,\n",
       "        -0.02219776, -0.25716725, -0.29525122, -0.30407235,  0.02016517,\n",
       "         0.09048087, -0.16293113,  0.11812694, -0.17674565,  0.8115082 ,\n",
       "         0.06450329,  0.20442279, -0.10483374,  0.09975157, -0.25057843,\n",
       "        -0.06292376,  0.17285128, -0.01758104, -0.30186537, -0.0841127 ,\n",
       "        -0.11114637,  0.05814372,  0.25236475, -0.19417268,  0.22315884,\n",
       "        -0.13729134, -0.06824694, -0.341236  ,  0.05999772,  0.41810328,\n",
       "        -0.20128277, -0.11010186, -0.16626425, -0.30353653,  0.11330597,\n",
       "         0.31198156,  0.11556248, -0.09846803, -0.30257723, -0.10206842,\n",
       "         0.03853445,  0.29365614, -0.0263411 ,  0.19250494, -0.18318594,\n",
       "        -0.387843  , -0.02986103, -0.07963722,  0.1678581 , -0.05191521,\n",
       "         0.00169835,  0.22374938,  0.2580453 , -0.16934054, -0.1468873 ,\n",
       "         0.03166825,  0.04612743, -0.26917952,  0.07807065, -0.12946828,\n",
       "        -0.40279043,  0.21611816,  0.04066769, -0.1153921 ,  0.13480614,\n",
       "         0.0533815 ,  0.01338466, -0.2828704 ,  0.02679291,  0.0553114 ,\n",
       "         0.2822382 ,  0.05319275, -0.03557036,  0.06398428,  0.01789244,\n",
       "        -0.12110488, -0.3023702 ,  0.15235233,  0.9217616 ,  0.28769338,\n",
       "        -0.37357402,  0.16022815,  0.04541649, -0.24403265,  0.05417794,\n",
       "         0.17239033,  0.12251239, -0.10201354, -0.07567043, -0.0966015 ,\n",
       "         0.05389016,  0.11833748,  0.44211835,  0.08047371,  0.05612666,\n",
       "        -0.17100687, -0.2711482 ,  0.11586259, -0.334083  ,  0.1370273 ,\n",
       "        -0.6890031 ,  0.03334224, -0.2867712 , -0.13632278, -0.04118263,\n",
       "        -0.3916107 , -0.03816559,  0.08995713, -0.2392849 ,  0.15313514,\n",
       "         0.23271668, -0.02023195,  0.19865698,  0.23517253, -0.0146532 ,\n",
       "        -0.14265741, -0.19406287,  0.13939749],\n",
       "       [ 1.2946371 , -0.11253475,  0.05029635,  0.2367229 , -0.13244802,\n",
       "         0.02443308, -0.14652136, -0.28027052, -0.18048939,  0.18805891,\n",
       "         0.13685365, -0.49752113, -0.09074111, -0.4175201 , -0.28877217,\n",
       "         0.05546521, -0.2556944 , -0.18724218, -0.31757542,  1.2265577 ,\n",
       "         0.05485158,  0.16542311,  0.07241534,  0.00525282, -0.06909394,\n",
       "        -0.30875286,  0.07455752, -0.02353438, -0.28600615,  0.07911482,\n",
       "         0.03179615,  0.06833802,  0.12678264,  0.03676327,  0.06749427,\n",
       "        -0.13177454, -0.20647688, -0.40936542,  0.04382695,  0.5071922 ,\n",
       "        -0.07591803, -0.01022388, -0.11367724, -0.03348291,  0.40537632,\n",
       "         0.01219334,  0.21148013, -0.15177499, -0.1675088 ,  0.25807852,\n",
       "        -0.06687336,  0.18238536, -0.07498309,  0.18307114, -0.02346368,\n",
       "        -0.1321392 ,  0.03790202,  0.05352983, -0.04517508, -0.09385905,\n",
       "        -0.10957684,  0.21261135,  0.01746937, -0.23257476,  0.09728883,\n",
       "        -0.06119744,  0.05879438, -0.5455181 ,  0.64510447,  0.20823778,\n",
       "        -0.26956758,  0.14502303,  0.26874134, -0.03323599,  0.23889118,\n",
       "         0.07796904, -0.03574307, -0.3639748 , -0.15900753,  0.04273356,\n",
       "         0.04090342,  0.03733777, -0.22868969, -0.05473918, -0.14085442,\n",
       "        -0.25791603, -0.17344423,  0.18025579,  0.5614433 ,  0.18826233,\n",
       "        -0.10364699,  0.17972913,  0.21095553, -0.1817403 ,  0.2866041 ,\n",
       "         0.18437123,  0.20135565, -0.23521572,  0.22913568,  0.18358126,\n",
       "        -0.09725957,  0.08446427,  0.34726897,  0.20610821, -0.05446661,\n",
       "        -0.32405758, -0.04476469,  0.09620295, -0.23533134,  0.06375794,\n",
       "        -0.32683748,  0.01521569, -0.45230308, -0.05667076, -0.21794087,\n",
       "        -0.13954318, -0.05025814, -0.07399976, -0.03221476, -0.13015044,\n",
       "         0.22862671, -0.3144746 ,  0.21375646, -0.1815645 , -0.01689448,\n",
       "        -0.11951823,  0.00593521,  0.12622672]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Embeddings\n",
    "\n",
    "The https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1 `SavedModel 2.0` has a vocabulary of 20,000 tokens and one out-of-vocabulary bucket for unknown tokens. The result is a tensor with 20,001 rows and 20 columns. The https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1 `SavedModel 2.0` has a vocabulary of 973,771 tokens and 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(973771, 128)\n"
     ]
    }
   ],
   "source": [
    "hub_layer_variables = hub_layer.variables\n",
    "print(hub_layer_variables[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfSbV6igl1EH"
   },
   "source": [
    "Let's now build the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 128)               124642688 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 124,644,769\n",
      "Trainable params: 124,644,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "The layers are stacked sequentially to build the classifier:\n",
    "\n",
    "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    "3. The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n",
    "\n",
    "Let's compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
    "\n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
    "\n",
    "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
    "\n",
    "Now, configure the model to use an optimizer and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0382 - accuracy: 0.9968 - val_loss: 0.3245 - val_accuracy: 0.8730\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0328 - accuracy: 0.9976 - val_loss: 0.3302 - val_accuracy: 0.8719\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0284 - accuracy: 0.9983 - val_loss: 0.3365 - val_accuracy: 0.8714\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0247 - accuracy: 0.9989 - val_loss: 0.3428 - val_accuracy: 0.8714\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0216 - accuracy: 0.9991 - val_loss: 0.3488 - val_accuracy: 0.8711\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0189 - accuracy: 0.9994 - val_loss: 0.3545 - val_accuracy: 0.8704\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0166 - accuracy: 0.9995 - val_loss: 0.3599 - val_accuracy: 0.8696\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0147 - accuracy: 0.9998 - val_loss: 0.3661 - val_accuracy: 0.8704\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0131 - accuracy: 0.9998 - val_loss: 0.3718 - val_accuracy: 0.8696\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0117 - accuracy: 0.9999 - val_loss: 0.3768 - val_accuracy: 0.8694\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0106 - accuracy: 0.9999 - val_loss: 0.3824 - val_accuracy: 0.8687\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0096 - accuracy: 0.9999 - val_loss: 0.3872 - val_accuracy: 0.8678\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0087 - accuracy: 0.9999 - val_loss: 0.3919 - val_accuracy: 0.8678\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0079 - accuracy: 0.9999 - val_loss: 0.3982 - val_accuracy: 0.8671\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0072 - accuracy: 0.9999 - val_loss: 0.4014 - val_accuracy: 0.8669\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0066 - accuracy: 0.9999 - val_loss: 0.4068 - val_accuracy: 0.8659\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0061 - accuracy: 0.9999 - val_loss: 0.4106 - val_accuracy: 0.8658\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.4155 - val_accuracy: 0.8657\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4193 - val_accuracy: 0.8657\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4235 - val_accuracy: 0.8655\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.8649\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4310 - val_accuracy: 0.8648\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4347 - val_accuracy: 0.8648\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.8646\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4419 - val_accuracy: 0.8645\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 0.8644\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4486 - val_accuracy: 0.8644\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4519 - val_accuracy: 0.8645\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.8644\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4582 - val_accuracy: 0.8642\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4611 - val_accuracy: 0.8640\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8643\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.8639\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.8640\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4726 - val_accuracy: 0.8642\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4752 - val_accuracy: 0.8637\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4781 - val_accuracy: 0.8641\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4805 - val_accuracy: 0.8635\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.8636\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.8631\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4882 - val_accuracy: 0.8631\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4905 - val_accuracy: 0.8632\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4929 - val_accuracy: 0.8631\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4953 - val_accuracy: 0.8630\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4975 - val_accuracy: 0.8628\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4999 - val_accuracy: 0.8630\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.8622\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5043 - val_accuracy: 0.8627\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5064 - val_accuracy: 0.8623\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8623\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5106 - val_accuracy: 0.8622\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8623\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 9.8682e-04 - accuracy: 1.0000 - val_loss: 0.5146 - val_accuracy: 0.8625\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 9.5646e-04 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.8624\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 9.2588e-04 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.8620\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 8.9652e-04 - accuracy: 1.0000 - val_loss: 0.5206 - val_accuracy: 0.8619\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 8.6906e-04 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8623\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 20s 2s/step - loss: 8.4268e-04 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.8619\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 8.1811e-04 - accuracy: 1.0000 - val_loss: 0.5264 - val_accuracy: 0.8623\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 7.9460e-04 - accuracy: 1.0000 - val_loss: 0.5280 - val_accuracy: 0.8619\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 7.7100e-04 - accuracy: 1.0000 - val_loss: 0.5300 - val_accuracy: 0.8619\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 7.4899e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8622\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 7.2813e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8618\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 7.0822e-04 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.8618\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 6.8881e-04 - accuracy: 1.0000 - val_loss: 0.5370 - val_accuracy: 0.8617\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 6.7013e-04 - accuracy: 1.0000 - val_loss: 0.5387 - val_accuracy: 0.8619\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 6.5208e-04 - accuracy: 1.0000 - val_loss: 0.5404 - val_accuracy: 0.8615\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 6.3534e-04 - accuracy: 1.0000 - val_loss: 0.5421 - val_accuracy: 0.8617\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 6.1888e-04 - accuracy: 1.0000 - val_loss: 0.5437 - val_accuracy: 0.8616\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 6.0296e-04 - accuracy: 1.0000 - val_loss: 0.5453 - val_accuracy: 0.8612\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 5.8768e-04 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.8616\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 5.7314e-04 - accuracy: 1.0000 - val_loss: 0.5485 - val_accuracy: 0.8614\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 5.5839e-04 - accuracy: 1.0000 - val_loss: 0.5501 - val_accuracy: 0.8615\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 5.4494e-04 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.8615\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 5.3189e-04 - accuracy: 1.0000 - val_loss: 0.5531 - val_accuracy: 0.8610\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 5.1920e-04 - accuracy: 1.0000 - val_loss: 0.5548 - val_accuracy: 0.8616\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 5.0682e-04 - accuracy: 1.0000 - val_loss: 0.5562 - val_accuracy: 0.8614\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 4.9527e-04 - accuracy: 1.0000 - val_loss: 0.5577 - val_accuracy: 0.8613\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 4.8321e-04 - accuracy: 1.0000 - val_loss: 0.5593 - val_accuracy: 0.8613\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 4.7273e-04 - accuracy: 1.0000 - val_loss: 0.5606 - val_accuracy: 0.8612\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 4.6179e-04 - accuracy: 1.0000 - val_loss: 0.5621 - val_accuracy: 0.8614\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 4.5149e-04 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.8612\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 4.4130e-04 - accuracy: 1.0000 - val_loss: 0.5650 - val_accuracy: 0.8613\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 19s 2s/step - loss: 4.3165e-04 - accuracy: 1.0000 - val_loss: 0.5664 - val_accuracy: 0.8613\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 4.2249e-04 - accuracy: 1.0000 - val_loss: 0.5678 - val_accuracy: 0.8613\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 4.1299e-04 - accuracy: 1.0000 - val_loss: 0.5692 - val_accuracy: 0.8614\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 4.0477e-04 - accuracy: 1.0000 - val_loss: 0.5705 - val_accuracy: 0.8612\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.9593e-04 - accuracy: 1.0000 - val_loss: 0.5719 - val_accuracy: 0.8613\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.8748e-04 - accuracy: 1.0000 - val_loss: 0.5733 - val_accuracy: 0.8616\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 3.7954e-04 - accuracy: 1.0000 - val_loss: 0.5746 - val_accuracy: 0.8610\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.7154e-04 - accuracy: 1.0000 - val_loss: 0.5759 - val_accuracy: 0.8612\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.6412e-04 - accuracy: 1.0000 - val_loss: 0.5772 - val_accuracy: 0.8611\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 3.5672e-04 - accuracy: 1.0000 - val_loss: 0.5786 - val_accuracy: 0.8612\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.4970e-04 - accuracy: 1.0000 - val_loss: 0.5798 - val_accuracy: 0.8612\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.4274e-04 - accuracy: 1.0000 - val_loss: 0.5811 - val_accuracy: 0.8612\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 3.3617e-04 - accuracy: 1.0000 - val_loss: 0.5823 - val_accuracy: 0.8611\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.2981e-04 - accuracy: 1.0000 - val_loss: 0.5837 - val_accuracy: 0.8612\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.2314e-04 - accuracy: 1.0000 - val_loss: 0.5848 - val_accuracy: 0.8611\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 3.1694e-04 - accuracy: 1.0000 - val_loss: 0.5861 - val_accuracy: 0.8611\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 3.1093e-04 - accuracy: 1.0000 - val_loss: 0.5873 - val_accuracy: 0.8610\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 3.0521e-04 - accuracy: 1.0000 - val_loss: 0.5886 - val_accuracy: 0.8611\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 2.9947e-04 - accuracy: 1.0000 - val_loss: 0.5898 - val_accuracy: 0.8611\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 2.9399e-04 - accuracy: 1.0000 - val_loss: 0.5910 - val_accuracy: 0.8611\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 2.8853e-04 - accuracy: 1.0000 - val_loss: 0.5922 - val_accuracy: 0.8611\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 2.8339e-04 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.8609\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 25s 3s/step - loss: 2.7830e-04 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.8610\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 20s 2s/step - loss: 2.7315e-04 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8611\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.6845e-04 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.8610\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 2.6366e-04 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.8611\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 2.5919e-04 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 0.8612\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 2.5458e-04 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.8612\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.5021e-04 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 0.8612\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.4572e-04 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 0.8611\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 24s 2s/step - loss: 2.4162e-04 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 0.8612\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.3766e-04 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.8611\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.3350e-04 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 0.8611\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.2983e-04 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 0.8611\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.2589e-04 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 0.8611\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 2.2232e-04 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 0.8611\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.1867e-04 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 0.8611\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.1513e-04 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.8610\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.1173e-04 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 0.8609\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.0827e-04 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 0.8611\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.0500e-04 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.8611\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 2.0182e-04 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.8609\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.9857e-04 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 0.8611\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.9553e-04 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 0.8612\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.9251e-04 - accuracy: 1.0000 - val_loss: 0.6184 - val_accuracy: 0.8609\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.8950e-04 - accuracy: 1.0000 - val_loss: 0.6194 - val_accuracy: 0.8611\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.8676e-04 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.8609\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.8386e-04 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 0.8609\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.8118e-04 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 0.8610\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.7846e-04 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 0.8611\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.7567e-04 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.8611\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.7312e-04 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 0.8611\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.7059e-04 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 0.8610\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.6809e-04 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.8611\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.6575e-04 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.8611\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.6331e-04 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 0.8608\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.6092e-04 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 0.8611\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.5867e-04 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 0.8610\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.5642e-04 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 0.8609\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.5419e-04 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 0.8611\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.5205e-04 - accuracy: 1.0000 - val_loss: 0.6338 - val_accuracy: 0.8609\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.4979e-04 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.8609\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.4779e-04 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.8610\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.4580e-04 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.8610\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.4373e-04 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 0.8609\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.4185e-04 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 0.8608\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.3995e-04 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 0.8608\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.3800e-04 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.8607\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.3613e-04 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.8608\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.3430e-04 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.8607\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.3251e-04 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.8607\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.3069e-04 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.8607\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.2901e-04 - accuracy: 1.0000 - val_loss: 0.6446 - val_accuracy: 0.8607\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.2746e-04 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.8606\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.2577e-04 - accuracy: 1.0000 - val_loss: 0.6464 - val_accuracy: 0.8607\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.2403e-04 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.8608\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.2246e-04 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 0.8606\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.2095e-04 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.8606\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.1943e-04 - accuracy: 1.0000 - val_loss: 0.6498 - val_accuracy: 0.8607\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.1790e-04 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.8608\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.1637e-04 - accuracy: 1.0000 - val_loss: 0.6515 - val_accuracy: 0.8608\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.1493e-04 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 0.8605\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.1344e-04 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 0.8607\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.1207e-04 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 0.8606\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.1079e-04 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 0.8605\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.0927e-04 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.8606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.0790e-04 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.8606\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.0666e-04 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8606\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.0531e-04 - accuracy: 1.0000 - val_loss: 0.6580 - val_accuracy: 0.8605\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.0403e-04 - accuracy: 1.0000 - val_loss: 0.6588 - val_accuracy: 0.8605\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.0278e-04 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 0.8606\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 1.0151e-04 - accuracy: 1.0000 - val_loss: 0.6605 - val_accuracy: 0.8604\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.0035e-04 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.8606\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 9.9173e-05 - accuracy: 1.0000 - val_loss: 0.6620 - val_accuracy: 0.8606\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 9.7883e-05 - accuracy: 1.0000 - val_loss: 0.6628 - val_accuracy: 0.8605\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 9.6792e-05 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 0.8604\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 9.5625e-05 - accuracy: 1.0000 - val_loss: 0.6645 - val_accuracy: 0.8603\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 9.4584e-05 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.8604\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 9.3488e-05 - accuracy: 1.0000 - val_loss: 0.6660 - val_accuracy: 0.8603\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 9.2390e-05 - accuracy: 1.0000 - val_loss: 0.6668 - val_accuracy: 0.8604\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 9.1345e-05 - accuracy: 1.0000 - val_loss: 0.6676 - val_accuracy: 0.8603\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 9.0278e-05 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.8603\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.9150e-05 - accuracy: 1.0000 - val_loss: 0.6691 - val_accuracy: 0.8603\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.8167e-05 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.8603\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 8.7205e-05 - accuracy: 1.0000 - val_loss: 0.6706 - val_accuracy: 0.8602\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.6229e-05 - accuracy: 1.0000 - val_loss: 0.6714 - val_accuracy: 0.8604\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.5196e-05 - accuracy: 1.0000 - val_loss: 0.6721 - val_accuracy: 0.8604\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.4245e-05 - accuracy: 1.0000 - val_loss: 0.6729 - val_accuracy: 0.8603\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.3277e-05 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.8603\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 8.2360e-05 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8600\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.1460e-05 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.8603\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 8.0599e-05 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.8603\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 7.9656e-05 - accuracy: 1.0000 - val_loss: 0.6766 - val_accuracy: 0.8604\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 7.8809e-05 - accuracy: 1.0000 - val_loss: 0.6773 - val_accuracy: 0.8602\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 7.7892e-05 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.8601\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 7.7086e-05 - accuracy: 1.0000 - val_loss: 0.6789 - val_accuracy: 0.8600\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 7.6225e-05 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.8602\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(1512),\n",
    "                    epochs=200,\n",
    "                    validation_data=validation_data.batch(1512),\n",
    "                    verbose=1, use_multiprocessing=True, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.742\n",
      "accuracy: 0.846\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, take a look [here](https://www.tensorflow.org/tutorials/keras/basic_text_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of text_classification_with_hub.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
