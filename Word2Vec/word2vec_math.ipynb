{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Stanford CS224N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "The principle of word2vec is that it is possible to establish the meaning of each word based on the contexts in which it is used. Meanings are encoded as dense vectors.\n",
    "\n",
    "Two words with similar vectors are expected to have similar meanings. \n",
    "\n",
    "## Two Algorithms: Skip-gram and Continuous Bag of Words\n",
    "Skip-gram is concerned with predicting context words given a center word. Continuous Bag of Words (CBOW) is concerned with predicting the center word given context words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus**\n",
    "A large body of continuous text.\n",
    "\n",
    "**Vocabulary**\n",
    "A vocabulary is a set of words in a corpus that vectors are calcuated for. Words that are rare in the corpus may not be included in the vocabulary.\n",
    "\n",
    "**Center Word**\n",
    "The word whose meaning is being computed.\n",
    "\n",
    "**Context Word**\n",
    "A word near the center word within a specified window.\n",
    "\n",
    "**Window**\n",
    "A number of word positions on either side of the center word. A window of size examines three words before the center word and three words to the right of the center word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Algorithm\n",
    "\n",
    "## Principle\n",
    "Go through each word position `t` in text. For the word at position `t` is a `center word`. Each word surrounding the center word is a `context word`. The algorithm considers words within a fixed distance of the center word. The fixed distance is a `window`. \n",
    "\n",
    "Let `c` be a center word and `o` be a context word or \"outside\" word.\n",
    "\n",
    "Use the similarity of word vectors for `c` and `o` to calculate `P(o|c)`. This is the probability of a word in context given a center word. Adjust the word vectors to maximize the probability. Note that the CBOW algorithm is concerned with `P(c|o)`. \n",
    "\n",
    "### Note on Initializing Vectors\n",
    "Vectors are initialized with random values. Initializing vectos with all zeros will yield poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Window of Size 2\n",
    "\n",
    "Let `w` be a word at the subscripted position. \n",
    "\n",
    "<font size=5>\n",
    "$P(w_{t-2}|w_{t}) \\; P(w_{t-1}|w_{t}) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  P(w_{t+1}|w_{t}) \\; P(w_{t+2}|w_{t}) $ <br>\n",
    "$\\;\\;\\; \\text{problems} \\;\\; \\text{turning} \\;\\;\\;\\;\\; \\text{into} \\;\\;\\;\\;\\; \\text{banking} \\;\\;\\;\\; \\text{crisis}$<br>\n",
    "$\\;\\;\\; \\text{window} \\;\\;\\;\\;  \\text{window} \\;\\;\\;\\; \\text{center} \\;\\; \\text{window} \\;\\;\\;\\; \\text{window}$<br>\n",
    "</font>\n",
    "<br>\n",
    "<font size=5>\n",
    "$ P(w_{t-2}|w_{t}) \\; P(w_{t-1}|w_{t}) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  P(w_{t+1}|w_{t}) \\; P(w_{t+2}|w_{t})    $<br>\n",
    "$\\;\\;\\; \\text{turning} \\;\\;\\;\\;\\; \\text{into} \\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{banking} \\;\\; \\text{crisis} \\;\\;\\;\\;\\;\\;\\; \\text{as}$<br>\n",
    "$\\;\\;\\; \\text{window} \\;\\;\\;\\;  \\text{window} \\;\\;\\;\\; \\text{center} \\;\\;\\;\\; \\text{window} \\;\\;\\;\\; \\text{window}$<br>\n",
    "</font>\n",
    "The goal is to maximize the product of these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Likelihood\n",
    "\n",
    "For each position `t = 1`, ..., `T`, predict context words within a window of fixed size `m`, given center word w[t]. The likelihood is a measure of how good the model is at predicting context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$T = \\text{number of words in a corpus} $ <br>\n",
    "$t = \\text{position of current word}  $ <br>\n",
    "$m = \\text{size of window, } \\pm m \\text{ words} $ <br>\n",
    "$w = \\text{word at a given position} $ <br>\n",
    "$\\theta = \\text{contents of the vector, which will be updated by the algorithm} $ <br>\n",
    "$L = \\text{likelihood}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the corpus, for each word in the window, get the probability that the context word is within the window of the given center word. This likelihood function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$ L(\\theta ) = \\prod_{t = 1}^{T}\\prod_{-m \\leq j \\leq m, \\;j \\ne 0}^{} P(w_{t+j}|w_{t};\\theta) $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram: Objective Function\n",
    "The objective function is the average negative log likelihood. It is also called the **cost function** or **loss function**. We minimize the objective function in order to maximize predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$ J(\\theta ) = -\\frac{1}{T}log  \\; L(\\theta) =  -\\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{-m \\leq j \\leq m, \\;j \\ne 0}^{} log \\; P(w_{t+j}|w_{t};\\theta) $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For center word `c` and context word `o`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$w = \\text{a given word} $ <br>\n",
    "$v_{w} = \\text{vector of word when word is a center word} $ <br>\n",
    "$u_{w} = \\text{vector of word when word is a context word} $ <br>\n",
    "$c = \\text{center word}  $ <br>\n",
    "$o = \\text{context word}  $ <br>\n",
    "$u_o^Tv_c = \\text{dot product} = u_o \\cdot v_c$ <br>\n",
    "$u_w^Tv_c = \\text{dot product} = u_w \\cdot v_c$ <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Context Word Given Center Word\n",
    "The objective function depends on the ability to calculate the probability of a context word given a center word.\n",
    "\n",
    "### The Probability\n",
    "<font size=5>\n",
    "$$ P(w_{t+j}|w_{t};\\theta) $$<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "We will use two vectors per word $w$:\n",
    "<font size=3>\n",
    "*  $v_w$ when w is a center word\n",
    "*  $u_w$ when w is a context word\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with `u` and `v` Notation\n",
    "\n",
    "<font size=5>\n",
    "$ P(u_{problems}|v_{into}) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P(u_{crisis}|v_{into})    $<br>\n",
    "$ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; P(u_{turning}|v_{into}) \\;\\;\\;\\;\\;\\;\\;  P(u_{banking}|v_{into}) \\; $<br>\n",
    "$\\text{problems \\;\\; turning \\;\\;\\;\\;\\; into \\;\\;\\;\\;\\; banking \\;\\;\\;\\; crisis}$<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation\n",
    "Then for a center word `c` and context word `o`:\n",
    "\n",
    "<font size=5>\n",
    "$$ P(o|c) = \\frac{exp(u_o^{\\intercal}v_c)}{\\sum_{w \\in V} exp(u_w^{\\intercal} v_c)} $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of the Equation\n",
    "\n",
    "<font size=3>\n",
    "$exp$: Exponentiation makes anything positive.<br>\n",
    "$u_o^{\\intercal}v_c$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Dot product compares similarity of `o` and `c`. It is the element-wise product.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $u^{\\intercal}v = u \\cdot v = \\sum_{i=1}^{n} u_iv_i$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Large dot product = larger probability<br>\n",
    "$\\sum_{w \\in V} exp(u_w^{\\intercal} v_c)$: Normalize over the entire vocabulary to give probability distribution.<br>\n",
    "$w \\in V$: If the word is a member of the vocabulary.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation as Softmax\n",
    "\n",
    "This is an example of the softmax function. The softmax function maps arbitrary values $x_{i}$ to a probability distribution $p_{i}$. It is called \"max\" because it amplifies the probability of the largest $x_{i}$ and \"soft\" because it still assigns some probability to smaller $x_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$softmax(x_i) =  \\frac{exp(x_{i})}{\\sum_{j=1}^{n} exp(x_{j})} =  p_{i}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02364054, 0.06426166, 0.1746813 , 0.474833  , 0.02364054,\n",
       "       0.06426166, 0.1746813 ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "np.exp(a) / np.sum(np.exp(a)) \n",
    "#array([0.02364054, 0.06426166, 0.1746813, 0.474833, 0.02364054,\n",
    "#       0.06426166, 0.1746813])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model: Compute `all` Vector Gradients\n",
    "Recall $\\theta$ represents all model parameters, in one long vector. **Remember that every word has two vectors**. In our case, with d-dimensional vectors and V-many words:\n",
    "\n",
    "<font size=5>\n",
    "$$\\theta = \n",
    "\\begin{bmatrix}\n",
    " v_{aardvark} \\\\ \n",
    " v_{a} \\\\ \n",
    " \\vdots \\\\ \n",
    " v_{zebra} \\\\ \n",
    " u_{aardvark} \\\\ \n",
    " u_{a} \\\\ \n",
    " \\vdots \\\\ \n",
    " u_{zebra} \\\\ \n",
    "\\end{bmatrix} = \\in \\mathbb{R}^{2dV}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Objective Function\n",
    "\n",
    "We need to minimize $J(\\theta)$. Let's start with the derivative of $J(\\theta)$ w.r.t $v_{c}$. Recall that $\\theta$ is our parameters. The parameters are the contents of the $u$ and $v$ vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial v_{c}} = \\frac{\\partial}{\\partial v_{c}} log \\frac{exp(u_{o}^{\\intercal} v_{c})}{\\sum_{w=1}^{V} exp(u_{o}^{\\intercal}v_{c})} $$\n",
    "</font>\n",
    "\n",
    "Apply the log rule that $log \\frac{a}{b} = log(a) - log(b)$\n",
    "\n",
    "<font size=5>\n",
    "$$= \\frac{\\partial}{\\partial v_{c}}(log(exp(u_{o}^{\\intercal}v_{c}))) - \\frac{\\partial}{\\partial v_{c}}(log \\sum_{w=1}^{V}exp(u_{o}^{\\intercal}v_{c}))$$\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left of the `-`\n",
    "\n",
    "Recall that $log(exp(x)) = x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$\\frac{\\partial}{\\partial v_{c}}(log(exp(u_{o}^{\\intercal}v_{c}))) = \\frac{\\partial}{\\partial v_{c}}(u_{o}^{\\intercal}v_{c}) = u_{o}$$\n",
    "</font>\n",
    "\n",
    "Note that $v_{c}$ is a vector, meaning that this is multivariate calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right of the `-`\n",
    "\n",
    "### Review\n",
    "First recall that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$\\frac{d}{dx}ln(x) = \\frac{1}{x}$<br>\n",
    "$\\frac{d}{dx}ln[f(x)] = \\frac{1}{f'(x)}$<br>\n",
    "$\\frac{d}{dx}\\sum x = \\sum \\frac{d}{dx}x$<br>\n",
    "Chain rule: $\\frac{d}{dx}f(g(x)) = f'g(x) * g'(x)$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the chain rule and move the derivative inside the summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$\\frac{\\partial}{\\partial v_{c}} = \\frac{1}{\\sum_{w=1}^{V}exp(u_{o}^{\\intercal}v_{c})}\\sum_{x=1}^{V}\\frac{\\partial}{\\partial v_{c}}exp(u_{x}^{\\intercal}v_{c})$$\n",
    "</font>\n",
    "\n",
    "Notice the \"change of variable\" to `x`. It is important use a different variable that we are summing over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the chain rule to the body of the inner summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$\\sum_{x=1}^{V} \\frac{exp(u_{x}^{\\intercal}v_{c})}{\\sum_{w=1}^{V}exp(u_{w}^{\\intercal}v_{c})} * u_{x} $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \n",
    "\n",
    "<font size=5>\n",
    "$$  \\frac{exp(u_{x}^{\\intercal}v_{c})}{\\sum_{w=1}^{V}exp(u_{w}^{\\intercal}v_{c})} = P(x|c)$$\n",
    "</font>\n",
    "\n",
    "is the form of the probability distribution discussed in the section above softmax. Therefore, \n",
    "\n",
    "<font size=5>\n",
    "$$ = \\sum_{x=1}^{V} P(x|c) * u_{x}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Two Sides of the `-`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$\\frac{\\partial}{\\partial v_{c}} log P(o|c) = u_{o} - \\sum_{x=1}^{V} P(x|c) * u_{x}$$\n",
    "</font>\n",
    "\n",
    "The meaning of this is that we take the observed representation of the context word $u_{o}$ and subtract from that the model thinks the context should look like. \n",
    "\n",
    "<font size=5>\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial v_{c}} = -u_{o} + \\sum_{x=1}^{V} P(x|c) * u_{x}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the Context Words\n",
    "\n",
    "The above was with respect to $v_{c}$, which means we did it for the center words. Now we need to do it for the context words, which us $u_{w}$.\n",
    "\n",
    "\n",
    "`if w not equal 0`, w is not the context word.\n",
    "<font size=5>$$ \\frac{\\partial J(\\theta)}{\\partial u_{w}} = \\sum_{x=1}^{V} P(x|c) * v_{c}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`if w equal 0`, w is the context word.\n",
    "<font size=5>\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial u_{w}} = -v + \\sum_{x=1}^{V} P(x|c) * v_{c}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both the derivatives, we can use them in our SGD equation to update the weights.\n",
    "\n",
    "However, there is one problem in this approach. As we can see, in the denominator, we have to take the exponential of the dot product of all our words and this is very time consuming when we have a huge vocabulary. We will need to train millions of weights which is not feasible.\n",
    "\n",
    "So, in order to increase the training time, a new method was used called negative sampling. In this method, we will be updating only a small percentage of weights in one step. We will select a few -ve words i.e. the words which are not in the context window and we will change our weights in such a way that it maximizes the probability of real context words and minimize the probability of random words appearing around the center word. This change the loss function and now we are trying to maximize the following equation:\n",
    "https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$J_{neg-sample}(o,v_{c},U) = -log(\\sigma(u_{o}^{\\intercal}\n",
    "v_{c})) - \\sum_{k=1}^{K} log(\\sigma(-u_{k}^{\\intercal} v_{c}))$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $K$ is the number of negative samples.\n",
    "\n",
    "To select random words, we use unigram distribution where more frequent words are more likely to get selected.\n",
    "\n",
    "To maximize the above term, we again need to take the derivative of the Loss function with respect to the weights, this case, it will be Uw, Uk, and Vc. Doing this in a similar way as we did above will give us the following three equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial v_{c}} = - \\sigma(-u_{o}^{\\intercal}\n",
    "v_{c})u_{o} + \\sum_{k=1}^{K} \\sigma(u_{k}^{\\intercal} v_{c})u_{k}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial u_{o}}  = - \\sigma(-u_{o}^{\\intercal}\n",
    "v_{c})v_{c} $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial u_{k}} = \\sum _{k=1}^{K} \\sigma(u_{k}^{\\intercal}v_{c})v_{c}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the derivatives calculated, now we can update our weight vectors little by little and get a vector representation that will point words appearing in a similar context in the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet  \n",
    "\\end{bmatrix} \\; \\;\\;\n",
    "\\begin{bmatrix}\n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet \\\\ \n",
    "\\bullet &\\bullet  &\\bullet  &\\bullet &\\bullet  \n",
    "\\end{bmatrix} \\; \\;\\; \\;\\;\\;\\;\n",
    "\\begin{bmatrix}\n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \n",
    "\\end{bmatrix} \\; \\;\\; \\;\\;\\;\\;\n",
    "\\begin{bmatrix}\n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \\\\ \n",
    "\\bullet \n",
    "\\end{bmatrix}$ <br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; U \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; V  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; U\\cdot v_{4}^{\\intercal} \\;\\;\\;\\;\\;\\;\\;\\; softmax(U \\cdot v_{4}^{\\intercal})$<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;outside \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; center \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; dot\\;product \\;\\;\\;\\; probabilities $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same predictions at each position. One-to-the-left = \"house\", Two-to-the-left = \"house\", One-to-the-right = \"house\" if \"house\" is most probable context word.\n",
    "\n",
    "We want a model that gives a reasonably high probability estimate to all words that occur in the context (fairly often)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3124",
   "language": "python",
   "name": "v3124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
